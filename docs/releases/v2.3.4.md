# ForenSynth AI — Release v2.3.4 “Polish”

Date: 2025-11-02  
Status: Stable  
Core script: `src/v2.3.4/forensynth_ai_v2_3_4_polish.py`

---

## Overview

v2.3.4 is a polish and UX pass on the v2.3.x line:

- Keeps the two-pass DFIR narrative (micro clusters → final executive merge).
- Refines visuals (donuts + heatmap + legends) for faster “at a glance” understanding.
- Adds sampling, a micro-governor, and real OpenAI cost reporting so large hunts stay practical.
- Ships a cleaner Evidence Appendix (CSV) for pivot-ready artifacts.

This is the version used in the 2025-11-02 POC run published under:

- `examples/2025-11-2-polish-run/`

---

## Key Changes

### 1. Visual polish (HTML report)

**Donut charts (By Phase)**

- Donut slices now map to MITRE-style phases:
  - Initial Access / Execution
  - Persistence / Privilege
  - Discovery / Lateral
  - Defense Evasion / Cleanup
  - Unmapped / Multiple (for noisy or mixed rules)
- Each slice:
  - Shows percentage of detections.
  - Has an exact count in the legend.
- Colors are kept consistent across:
  - Donut charts
  - “By Phase” references in the text
  - Any future stacked/heat visuals.

**Detection heatmap**

- “Event ID vs Day” heatmap retained, but:
  - Uses a unified blue palette to match donut charts.
  - Includes a short caption, e.g.:
    - Bursts after 21:00 UTC on 2025-10-10 and 2025-10-13 reflect peak encoded PowerShell + LOLBIN activity.
  - Adds a tiny Event ID footnote, for example:
    - 1 = Process Create (Sysmon)  
    - 11 = File / Task Create  
    - 13 = Registry Change  
    - 4104 = PowerShell ScriptBlock
- Legend fonts slightly increased for better readability on DFIR VMs.

**Scope + footers**

- Report footer now calls out:
  - Detections (raw): N
  - Detections (sampled): M
  - Selected micros: X / Y blocks
- Makes sampling and summarization scope explicit for responders.

---

### 2. Sampling + micro-governor (large hunts)

To keep 2k–3k+ detection hunts practical, v2.3.4 adds a light “micro governor” on top of Chainsaw output.

New arguments:

- `--limit-detections N`  
  Hard cap on how many detections are passed into the AI summarization layer.

- `--sample-step K`  
  Sample every K-th detection (stratified by original ordering) before chunking into micro clusters.

- `--max-input-tokens N`  
  Upper bound for total prompt tokens across micro summaries to avoid massive runs.

At runtime, the console prints a clear banner, for example:

- Sampling applied: 2705 → 902 (step=3, limit=1000)

This allows:

- Full Chainsaw hunt against 2k–3k detections.
- Summarization on a representative slice (for example every 3rd hit, capped at 1000).
- Stable runtime and predictable costs on GPT-5 merges.

---

### 3. Evidence Appendix CSV + HTML tables

Evidence export is now more structured and easier to pivot on.

**CSV export**

- `--export-evidence-csv`:
  - Writes a CSV Evidence Appendix alongside the report, for example:
    - `evidence_appendix.csv` within the run folder.
- Typical columns:
  - `timestamp_utc`
  - `rule_name`
  - `event_id`
  - `phase` (mapped)
  - `account`
  - `host` (if present)
  - `persistence_type` (task / service / run-key, etc.)
  - `notes`

**Inline HTML mini-tables**

- Report now includes small, skimmable tables such as:
  - Top Rules (by count)
  - Top Event IDs (by count)
  - Persistence Artifacts (counts)
  - Entities & Scope (for example):
    - Unique users
    - Local accounts created (≥N)
    - Group membership changes
    - Scheduled Tasks
    - Run-key writes
    - Services / COM hijacks

These tables mirror what you’d expect in consultancy-grade DFIR reports but sourced directly from your hunt.

---

### 4. Real cost reporting (OpenAI usage)

v2.3.4 switches from estimated cost to usage-based cost.

- Uses `response.usage.prompt_tokens` and `response.usage.completion_tokens` for:
  - `gpt-5-mini` micro summaries
  - `gpt-5` final merge
- Stores cumulative usage and prints a more accurate breakdown such as:
  - gpt-5-mini: in = X, out = Y → $A
  - gpt-5: in = P, out = Q → $B
  - Total cost: $C

This closely matches what appears in the OpenAI usage dashboard.

---

### 5. CLI ergonomics and guardrails

The v2.3.4 polish script (`forensynth_ai_v2_3_4_polish.py`) adds or tightens:

**Dotenv handling**

- Secure `.env` loading with basic permission checks.
- Reads `OPENAI_API_KEY` and exits with a clear message if missing.

**Key arguments**

- Core flow:
  - `--two-pass`
  - `--integrity on|off`
  - `--chunk-size N`
  - `--micro-workers N`
  - `--rpm N` (0 means no client-enforced RPM cap)
  - `--max-input-tokens N`
- Sampling:
  - `--limit-detections N`
  - `--sample-step N`
- Output controls:
  - `--make-html`
  - `--branding on|off`
  - `--toc on|off`
  - `--chart-style bar|donut|both`
  - `--export-evidence-csv`
- Retry and timeout defaults tuned for home-lab stability.

---

## Recommended Profiles (v2.3.4)

Fast POC (≤ 1000 detections):

- Two-pass
- `micro-workers` around 3
- Higher `rpm` (for example 90)
- `chunk-size` around 80
- `integrity on`
- `max-input-tokens` 60000
- `limit-detections` 1000
- `sample-step` 3
- HTML + branding + TOC
- `chart-style` both
- `--export-evidence-csv`

Heavier hunts (2000–3000 detections, sampled):

- Same base flags as POC, with slightly lower `rpm` (for example 60) and `chunk-size` around 60.
- Always include `limit-detections` and `sample-step` to keep runs under control.

(Exact values can be tuned by environment.)

---

## Notes and Limitations

- Sampling is opt-in.  
  If `--limit-detections` and `--sample-step` are omitted, the script will summarize all detections. Runtime and cost will scale with total detections.

- Scope transparency is intentional.  
  Footers and “At a Glance” sections explicitly mention when sampling is applied.

- This is a home-lab / POC tool, not a product:
  - Assumes EVTX + Chainsaw workflow.
  - Requires an OpenAI API key and basic awareness of token and usage costs.

---

## Upgrade Path

From v2.3.3 (Visual Refresh):

- Add `src/v2.3.4/forensynth_ai_v2_3_4_polish.py`.
- Update:
  - `README.md`
  - `CHANGELOG.md`
  - `docs/releases/v2.3.4.md` (this file)
- Optionally keep v2.3.3 as a simpler visual-only baseline script.

From v2.2.x or older:

- Migrate directly to v2.3.4:
  - Two-pass summarization and visuals are now the recommended path.
  - Review CLI flags; several new arguments control sampling and evidence export.

---
