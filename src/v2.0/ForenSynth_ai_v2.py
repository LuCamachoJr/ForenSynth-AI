#!/usr/bin/env python3
# ForenSynth AI v2.0 — DFIR Intelligence Engine
from __future__ import annotations

import argparse
import json
import math
import os
import random
import re
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pypandoc

# External libs
import tiktoken
from dotenv import load_dotenv
from openai import (
    APIConnectionError,
    APIError,
    APITimeoutError,
    BadRequestError,
    OpenAI,
    RateLimitError,
)
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn, TimeElapsedColumn
from rich.table import Table

console = Console()

# ─────────────────────────────────────────────────────────────────────────────
# Config & Defaults
# ─────────────────────────────────────────────────────────────────────────────
PRICING = {
    "gpt-5-mini": {"in": 0.00025, "out": 0.00200},
    "gpt-5": {"in": 0.00125, "out": 0.01000},
    "gpt-4o-mini": {"in": 0.00015, "out": 0.00060},  # optional alt
}

DEFAULT_CHUNK_MODEL = os.getenv("CHUNK_MODEL", "gpt-5-mini")
DEFAULT_FINAL_MODEL = os.getenv("FINAL_MODEL", "gpt-5")

DEFAULT_SYSTEM = (
    "You are a senior DFIR analyst. Produce concise, accurate summaries. "
    "Group related detections, highlight notable TTPs/tooling, dedupe repetition, "
    "and end with actionable recommendations prioritized by risk and effort."
)

FINAL_SYSTEM = (
    "You are a DFIR lead. Merge the provided micro-summaries into a coherent executive report. "
    "Eliminate repetition; group by phases/TTPs; quantify scope where possible; "
    "end with prioritized, actionable recommendations (High/Med/Low) and quick wins."
)

HTML_CSS = """
:root{--bg:#0c1220;--card:#111a2b;--ink:#e6f0ff;--muted:#9bb0cf;--accent:#6ea8fe;--chip:#0f274e;}
*{box-sizing:border-box}
body{margin:24px;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto;background:var(--bg);color:var(--ink)}
.container{max-width:1100px;margin:0 auto}
.card{background:var(--card);border-radius:14px;padding:18px 20px;margin-bottom:16px;border:1px solid #182644}
h1,h2,h3{margin:0 0 12px 0}
h1{font-size:28px}
h2{font-size:22px;border-bottom:1px dashed #213254;padding-bottom:6px}
h3{font-size:18px;margin-top:10px}
p{line-height:1.55}
.badges{display:flex;flex-wrap:wrap;gap:8px;margin-top:8px}
.badge{background:var(--chip);color:#cfe1ff;border:1px solid #1d315d;border-radius:999px;padding:6px 10px;font-size:12px}
.code{background:#0b152b;border:1px solid #14254b;padding:10px;border-radius:8px;overflow:auto}
.small{color:var(--muted);font-size:13px}
a{color:var(--accent);text-decoration:none}
a:hover{text-decoration:underline}
table{border-collapse:collapse;width:100%}
td,th{border:1px solid #203156;padding:8px 10px}
.section{margin-top:10px}
.footer{margin-top:24px;color:var(--muted);font-size:13px;text-align:center}
.toc a{display:block;margin:6px 0}
.figure{background:#0b152b;border:1px solid #19294b;border-radius:12px;padding:12px}
canvas{width:100%;max-width:100%}
"""

HTML_TEMPLATE = """<!doctype html>
<html>
<head>
<meta charset="utf-8"/>
<title>ForenSynth AI Report — {stamp}</title>
<style>{css}</style>
</head>
<body>
<div class="container">
  <div class="card">
    <h1>ForenSynth AI Report — {human_stamp}</h1>
    <div class="small">Automated DFIR Intelligence Summary</div>
    <div class="badges">
      <span class="badge">Detections: {det_count}</span>
      <span class="badge">Models: micro={chunk_model}, final={final_model}</span>
      <span class="badge">Runtime: {runtime}s</span>
      <span class="badge">Cost: ${cost}</span>
      <span class="badge">Started {start_utc} · Finished {end_utc}</span>
    </div>
  </div>

  {toc_html}

  <div class="card">
    <h2 id="exec">Executive Summary</h2>
    {final_html}
  </div>

  <div class="card figure">
    <h2 id="heatmap">Detections Heatmap (per-hour)</h2>
    <canvas id="heat"></canvas>
    <div class="small">Shows hourly intensity over the log timeframe.</div>
  </div>

  <div class="card">
    <h2 id="ioc">Indicators of Compromise</h2>
    {ioc_html}
  </div>

  <div class="card">
    <h2 id="appendix">Appendix: Micro-Summaries</h2>
    {micros_html}
  </div>

  {branding_html}

  <div class="footer">Generated by ForenSynth AI — DFIR Intelligence Engine</div>
</div>

<script>{chart_js}</script>
<script>
const data = {heatmap_json};
const labels = data.labels;
const values = data.values;
const ctx = document.getElementById('heat').getContext('2d');
new Chart(ctx, {{
  type: 'matrix',
  data: {{
    datasets: [{{
      label: 'Detections/hour',
      data: values.map(v=>({{x:v.x,y:v.y,v:v.v}})),
      backgroundColor: ctx => {{
        const v = ctx.raw.v;
        const a = Math.min(1, 0.08 + v/Math.max(1,data.max)*0.92);
        return `rgba(110,168,254,${{a}})`;
      }},
      borderColor: '#1c2d52',
      borderWidth: 1,
      width: ({chart_w}) => {chart_w.chart.chartArea.width / data.w},
      height: ({chart_h}) => {chart_h.chart.chartArea.height / data.h},
    }}]
  }},
  options: {{
    plugins: {{
      legend: {{display:false}},
      tooltip: {{
        callbacks: {{
          title: (items)=>`Hour ${items[0].raw.x}:00`,
          label: (item)=>`Count: ${item.raw.v}`
        }}
      }}
    }},
    scales: {{
      x: {{ ticks: {{color:'#9bb0cf'}}, grid: {{color:'#152445'}}, title: {{display:true,text:'Hour (0-23)',color:'#9bb0cf'}} }},
      y: {{ ticks: {{color:'#9bb0cf'}}, grid: {{color:'#152445'}}, title: {{display:true,text:'Day Index',color:'#9bb0cf'}} }}
    }}
  }}
}});
</script>
</body></html>"""

CHART_JS_MIN = r"""
/*!
Chart.js v4.4.1 (matrix plugin minimal)
*/
class MatrixController extends Chart.DatasetController{initialize(){super.initialize()}parseObjectData(meta,data,metaDataFn){const parsed=[];for(const obj of data){parsed.push({x:+obj.x,y:+obj.y,_custom:{v:+obj.v}})}return parsed}updateElements(e,start,mode){const me=this;for(let i=0;i<e.length;i++){const r=e[i],idx=start+i,props=me.resolveDataElementOptions(idx,mode);Object.assign(r,props,me._resolveElementGeometry(idx))}}_resolveElementGeometry(i){const v=this.getParsed(i),x=this.chart.scales.x.getPixelForValue(v.x),y=this.chart.scales.y.getPixelForValue(v.y),w=this.chart.chartArea.width/this._cachedMeta.data.length,h=this.chart.chartArea.height/this._cachedMeta.data.length;return{x:x,y:y,width:w,height:h}}}
MatrixController.id="matrix";MatrixController.defaults={dataElementType: 'rectangle'};Chart.register(MatrixController);
"""


# ─────────────────────────────────────────────────────────────────────────────
@dataclass
class AppConfig:
    two_pass: bool
    fast: bool
    make_html: bool
    branding: str
    toc: str

    evtx_root: Path
    out_root: Path
    rules_dir: Path
    mapping_path: Path
    prefer_logs: List[str]

    chunk_model: str
    final_model: str
    chunk_size: int
    max_chunks: int
    max_input_tokens: int
    micro_truncate: int
    micro_include_script: bool
    rpm: int
    micro_workers: int
    llm_timeout: int
    llm_retries: int
    temperature: float


def ok(msg):
    console.print(Panel.fit(f"[green]✔ {msg}[/green]"))


def info(msg):
    console.print(Panel.fit(f"[yellow]⚙ {msg}[/yellow]"))


def die(msg):
    console.print(Panel.fit(f"[red]✘ {msg}[/red]"))
    sys.exit(1)


# ─────────────────────────────────────────────────────────────────────────────
# Helpers
# ─────────────────────────────────────────────────────────────────────────────
def get_encoder():
    try:
        return tiktoken.get_encoding("cl100k_base")
    except Exception:
        return tiktoken.get_encoding("cl100k_base")


def est_tokens(enc, text):
    try:
        return len(enc.encode(text))
    except Exception:
        return math.ceil(len(text) / 4)


def chunk_list(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i : i + n]


def now_utc_iso():
    return datetime.now(timezone.utc).isoformat(timespec="seconds").replace("+00:00", "Z")


# ─────────────────────────────────────────────────────────────────────────────
# Chainsaw
# ─────────────────────────────────────────────────────────────────────────────
def find_latest_container(root: Path) -> Path:
    if not root.exists():
        die(f"EVTX root not found: {root}")
    ds = [p for p in root.iterdir() if p.is_dir()]
    if not ds:
        die(f"No subfolders under {root}")
    return max(ds, key=lambda p: p.stat().st_mtime)


def resolve_evtx_source(root: Path, prefer_logs: List[str]) -> Tuple[Path, Optional[Path]]:
    latest = find_latest_container(root)
    ok(f"Using latest EVTX directory: {latest}")
    # choose file if preferred exists
    for name in prefer_logs:
        p = latest / name
        if p.is_file():
            ok(f"Selected log file: {name}")
            return latest, p
    return latest, None


def run_chainsaw(
    src_dir: Path, out_json: Path, rules: Path, mapping: Path, single_file: Optional[Path] = None
) -> Tuple[int, List[str]]:
    console.print(Panel.fit("🪓 Chainsaw Module Active — Sigma Hunt in Progress…"))
    cmd = [
        "chainsaw",
        "hunt",
        str(single_file or src_dir),
        "--mapping",
        str(mapping),
        "--rule",
        str(rules),
        "-s",
        str(rules.parent),
        "--json",
        "--output",
        str(out_json),
    ]
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold]Hunting[/bold]"),
        BarColumn(),
        TimeElapsedColumn(),
        transient=True,
    ) as prog:
        task = prog.add_task("hunt", total=None)
        try:
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        finally:
            prog.update(task, advance=1)
    # read first few bytes
    try:
        txt = out_json.read_text(encoding="utf-8")
        data = json.loads(txt)
    except Exception:
        data = []
    dets = len(data) if isinstance(data, list) else 0
    ok("Chainsaw hunt completed. Detections: {}".format(dets if dets else "?"))
    return dets, cmd


# ─────────────────────────────────────────────────────────────────────────────
# Detections parsing + prompts
# ─────────────────────────────────────────────────────────────────────────────
def load_detections(path: Path, max_items: int) -> List[Dict[str, Any]]:
    text = path.read_text(encoding="utf-8")
    obj = json.loads(text)
    if not isinstance(obj, list):
        raise RuntimeError("Unexpected Chainsaw JSON (expected list).")
    if max_items > 0:
        obj = obj[:max_items]
    return obj


def detection_timestamp_iso(det: Dict[str, Any]) -> Optional[str]:
    try:
        return det["document"]["data"]["Event"]["System"]["TimeCreated_attributes"]["SystemTime"]
    except Exception:
        return det.get("timestamp")


def build_micro_prompt(block: List[Dict[str, Any]], include_script: bool, micro_trunc: int) -> str:
    header = (
        "Micro-summarize for DFIR triage in <= 10 bullets total. "
        "Group similar items, name key TTPs (MITRE IDs if present), counts/timestamps if available. "
        "Output:\n• Executive bullets\n• Key TTPs\n• Notable IOCs\n"
    )
    parts = []
    for d in block:
        ts = detection_timestamp_iso(d) or "N/A"
        rule = d.get("name", "N/A")
        tags = ", ".join(d.get("tags", []) or []) or "None"
        snippet = ""
        if include_script and micro_trunc > 0:
            script = (
                (((d.get("document") or {}).get("data") or {}).get("Event") or {})
                .get("EventData", {})
                .get("ScriptBlockText", "")
            )
            if isinstance(script, str) and script:
                snippet = script[:micro_trunc] + (
                    "… [truncated]" if len(script) > micro_trunc else ""
                )
        line = f"- [{ts}] {rule} (Tags: {tags})"
        if snippet:
            line += f" | snippet: {snippet}"
        parts.append(line)
    return header + "\n".join(parts)


def build_final_prompt(micros: List[str]) -> str:
    return (
        "Merge the following micro-summaries into a single executive DFIR report.\n"
        "Eliminate duplicates, group themes, and produce:\n"
        "1) Executive Summary\n2) Observed Activity (grouped)\n3) Key TTPs/Techniques\n"
        "4) Indicators of Compromise\n5) Actionable Recommendations (High/Med/Low)\n\n"
        + "\n\n---\n\n".join(micros)
    )


# ─────────────────────────────────────────────────────────────────────────────
# LLM
# ─────────────────────────────────────────────────────────────────────────────
def backoff_sleep(i: int):
    time.sleep(min(30.0, (1.5**i) + random.uniform(0, 0.3)))


def call_llm(
    client: OpenAI,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float,
    timeout_s: int,
    retries: int,
) -> str:
    safe_temp = 1.0 if model.startswith("gpt-5") else temperature
    last = None
    for i in range(retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=safe_temp,
                timeout=timeout_s,
            )
            return resp.choices[0].message.content or ""
        except BadRequestError as e:
            if "temperature" in str(e):
                safe_temp = 1.0
                continue
            raise
        except (RateLimitError, APIConnectionError, APITimeoutError, APIError) as e:
            last = e
            backoff_sleep(i)
            continue
    raise RuntimeError(f"LLM retries exceeded: {last}")


# ─────────────────────────────────────────────────────────────────────────────
# Heatmap (hourly)
# ─────────────────────────────────────────────────────────────────────────────
def build_heatmap(dets: List[Dict[str, Any]]) -> Dict[str, Any]:
    times = []
    for d in dets:
        ts = detection_timestamp_iso(d)
        if not ts:
            continue
        try:
            dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
        except Exception:
            continue
        times.append(dt.astimezone(timezone.utc))
    if not times:
        return {"labels": [], "values": [], "w": 24, "h": 1, "max": 0}
    # map day index + hour
    base_day = min(t.date() for t in times)

    def idx(t):
        return (t.date() - base_day).days

    buckets = {}
    maxv = 0
    for t in times:
        key = (idx(t), t.hour)
        buckets[key] = buckets.get(key, 0) + 1
        maxv = max(maxv, buckets[key])
    h = (max(k[0] for k in buckets.keys()) + 1) if buckets else 1
    w = 24
    values = [
        {"x": hr, "y": day, "v": buckets.get((day, hr), 0)} for day in range(h) for hr in range(w)
    ]
    return {"labels": [], "values": values, "w": w, "h": h, "max": maxv}


# ─────────────────────────────────────────────────────────────────────────────
# IOC extraction (simple regex)
# ─────────────────────────────────────────────────────────────────────────────
RE_IP = re.compile(r"\b(?:\d{1,3}\.){3}\d{1,3}\b")
RE_DOM = re.compile(
    r"\b[a-zA-Z0-9.-]+\.(?:com|net|org|co|io|dev|gov|mil|edu|info|biz|ru|cn|uk|de|jp)\b", re.I
)
RE_MD5 = re.compile(r"\b[a-f0-9]{32}\b", re.I)
RE_SHA1 = re.compile(r"\b[a-f0-9]{40}\b", re.I)
RE_SHA256 = re.compile(r"\b[a-f0-9]{64}\b", re.I)


def ioc_from_text(txt: str) -> Dict[str, set]:
    return {
        "ip": set(RE_IP.findall(txt)),
        "domain": set(RE_DOM.findall(txt)),
        "md5": set(RE_MD5.findall(txt)),
        "sha1": set(RE_SHA1.findall(txt)),
        "sha256": set(RE_SHA256.findall(txt)),
    }


def aggregate_iocs(dets: List[Dict[str, Any]]) -> Dict[str, set]:
    acc = {"ip": set(), "domain": set(), "md5": set(), "sha1": set(), "sha256": set()}
    for d in dets:
        data = json.dumps(d, ensure_ascii=False)
        found = ioc_from_text(data)
        for k in acc:
            acc[k].update(found[k])
    return acc


def ioc_html_table(iocs: Dict[str, set]) -> str:
    rows = []
    for k in ["ip", "domain", "md5", "sha1", "sha256"]:
        vals = sorted(iocs[k])[:300]
        if not vals:
            continue
        rows.append(
            f"<tr><th>{k.upper()}</th><td><div class='code'>{'<br>'.join(map(str, vals))}</div></td></tr>"
        )
    return (
        "<table>"
        + ("".join(rows) if rows else "<tr><td>No IOCs detected from text extraction.</td></tr>")
        + "</table>"
    )


# ─────────────────────────────────────────────────────────────────────────────
# Reports + CSV log
# ─────────────────────────────────────────────────────────────────────────────
def write_html(outdir: Path, html: str) -> Path:
    out = outdir / f"forensynth_report_{datetime.now(timezone.utc).strftime('%Y-%m-%d')}.html"
    out.write_text(html, encoding="utf-8")
    return out


def write_md(outdir: Path, md: str) -> Path:
    out = outdir / f"forensynth_summary_{datetime.now(timezone.utc).strftime('%Y-%m-%d')}.md"
    out.write_text(md, encoding="utf-8")
    return out


def write_csv_runlog(root: Path, row: Dict[str, Any]):
    path = root / "run_log.csv"
    rows = []
    if path.exists():
        for ln in path.read_text(encoding="utf-8").splitlines():
            if ln.strip() and not ln.startswith("timestamp,"):
                parts = ln.split(",")
                rows.append(parts)
    # add new
    rows.append(
        [
            row["timestamp"],
            str(row["detections"]),
            str(row["runtime_sec"]),
            f"{row['cost_usd']:.6f}",
            row["integrity"],
            row["chunk_model"],
            row["final_model"],
        ]
    )
    # sort desc by timestamp
    rows.sort(key=lambda r: r[0], reverse=True)
    # write
    head = "timestamp,detections,runtime_sec,cost_usd,integrity,chunk_model,final_model\n"
    path.write_text(head + "\n".join(",".join(r) for r in rows), encoding="utf-8")

    # Pretty print last 5
    t = Table(title="Recent ForenSynth Runs (latest 5)")
    t.add_column("Timestamp", style="cyan")
    t.add_column("Det", justify="right")
    t.add_column("Sec", justify="right")
    t.add_column("$", justify="right")
    t.add_column("Integrity")
    t.add_column("Micro")
    t.add_column("Final")
    for r in rows[:5]:
        t.add_row(r[0], r[1], r[2], r[3], r[4], r[5], r[6])
    console.print(t)
    return path


# ─────────────────────────────────────────────────────────────────────────────
# Two-pass summarization with live progress
# ─────────────────────────────────────────────────────────────────────────────
def two_pass(
    client: OpenAI, dets: List[Dict[str, Any]], cfg: AppConfig
) -> Tuple[str, Dict[str, Tuple[int, int]]]:
    enc = get_encoder()
    # chunk
    blocks = list(chunk_list(dets, cfg.chunk_size))
    micros = [""] * len(blocks)
    usage: Dict[str, Tuple[int, int]] = {}
    micro_in = micro_out = 0

    console.print(
        Panel.fit("Detections found ({}) — generating micro-summaries…".format(len(dets)))
    )
    with Progress(
        TextColumn("[bold]Micro[/bold]"),
        BarColumn(),
        TextColumn("{task.completed}/{task.total}"),
        TimeElapsedColumn(),
    ) as prog:
        task = prog.add_task("micro", total=len(blocks))

        def work(i: int, blk: List[Dict[str, Any]]):
            prompt = build_micro_prompt(blk, cfg.micro_include_script, cfg.micro_truncate)
            tin = est_tokens(enc, DEFAULT_SYSTEM) + est_tokens(enc, prompt)
            txt = call_llm(
                client,
                cfg.chunk_model,
                DEFAULT_SYSTEM,
                prompt,
                cfg.temperature,
                cfg.llm_timeout,
                cfg.llm_retries,
            )
            tout = est_tokens(enc, txt)
            return i, txt, tin, tout

        futures = []
        max_workers = max(1, cfg.micro_workers)
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            for i, blk in enumerate(blocks):
                futures.append(ex.submit(work, i, blk))
            for f in as_completed(futures):
                i, txt, tin, tout = f.result()
                micros[i] = f"### Micro {i + 1}\n\n{txt}\n"
                micro_in += tin
                micro_out += tout
                prog.update(task, advance=1)

    usage[cfg.chunk_model] = (micro_in, micro_out)

    # final merge
    console.print(Panel.fit("Compiling executive summary with final model…"))
    user = build_final_prompt(micros)
    fin_in = est_tokens(enc, FINAL_SYSTEM) + est_tokens(enc, user)
    final = call_llm(
        client,
        cfg.final_model,
        FINAL_SYSTEM,
        user,
        cfg.temperature,
        cfg.llm_timeout,
        cfg.llm_retries,
    )
    fin_out = est_tokens(enc, final)
    usage[cfg.final_model] = (
        usage.get(cfg.final_model, (0, 0))[0] + fin_in,
        usage.get(cfg.final_model, (0, 0))[1] + fin_out,
    )

    md = (
        "# Final Executive Report\n\n"
        + final
        + "\n\n---\n\n## Appendix: Micro-Summaries\n\n"
        + "\n".join(micros)
    )
    return md, usage


# ─────────────────────────────────────────────────────────────────────────────
# HTML assembly
# ─────────────────────────────────────────────────────────────────────────────
def as_html(md: str) -> str:
    try:
        return pypandoc.convert_text(md, to="html", format="gfm")
    except Exception:
        return (
            "<pre class='code'>Pandoc not available; showing Markdown:</pre><div class='code'>"
            + md.replace("<", "&lt;")
            + "</div>"
        )


def assemble_html(
    final_html: str,
    micros_html: str,
    ioc_html: str,
    heat_json: Dict[str, Any],
    det_count: int,
    chunk_model: str,
    final_model: str,
    runtime: int,
    cost: float,
    start_utc: str,
    end_utc: str,
    branding: str,
    toc_on: bool,
) -> str:
    branding_html = (
        "" if branding == "off" else "<div class='footer'>Powered by ForenSynth AI™</div>"
    )
    toc_html = (
        ""
        if not toc_on
        else (
            "<div class='card toc'><h2>Index</h2>"
            "<a href='#exec'>Executive Summary</a>"
            "<a href='#heatmap'>Detections Heatmap</a>"
            "<a href='#ioc'>Indicators of Compromise</a>"
            "<a href='#appendix'>Appendix: Micro-Summaries</a></div>"
        )
    )
    return HTML_TEMPLATE.format(
        css=HTML_CSS,
        stamp=start_utc,
        human_stamp=start_utc.replace("T", " "),
        det_count=det_count,
        chunk_model=chunk_model,
        final_model=final_model,
        runtime=runtime,
        cost=f"{cost:.6f}",
        start_utc=start_utc,
        end_utc=end_utc,
        branding_html=branding_html,
        final_html=final_html,
        micros_html=micros_html,
        ioc_html=ioc_html,
        chart_js="const Chart=window.Chart;"
        + CHART_JS_MIN,  # minimal embed; assumes Chart already available
        heatmap_json=json.dumps(heat_json),
        chart_w="ctx",
        chart_h="ctx",
        toc_html=toc_html,
    )


# ─────────────────────────────────────────────────────────────────────────────
# Cost
# ─────────────────────────────────────────────────────────────────────────────
def estimate_cost(usage: Dict[str, Tuple[int, int]]) -> Tuple[float, List[str]]:
    total = 0.0
    lines = []
    for m, (tin, tout) in usage.items():
        price = PRICING.get(m, {"in": 0, "out": 0})
        cost = (tin / 1000.0) * price["in"] + (tout / 1000.0) * price["out"]
        total += cost
        lines.append(
            f"- {m}: in={tin}, out={tout} → ${cost:.6f} (in {price['in']}/k, out {price['out']}/k)"
        )
    return round(total, 6), lines


# ─────────────────────────────────────────────────────────────────────────────
# CLI / Main
# ─────────────────────────────────────────────────────────────────────────────
def parse_args() -> AppConfig:
    p = argparse.ArgumentParser(description="ForenSynth AI v2.0 — DFIR Intelligence Engine")
    p.add_argument("--two-pass", action="store_true")
    p.add_argument("--fast", action="store_true")
    p.add_argument("--make-html", action="store_true")
    p.add_argument("--branding", choices=["on", "off"], default="on")
    p.add_argument("--toc", choices=["on", "off"], default="off")

    p.add_argument("--evtx-root", type=Path, default=Path("/mnt/evtx_share/DFIR-Lab-Logs"))
    p.add_argument(
        "--out-root", type=Path, default=Path.home() / "DFIR-Labs" / "ForenSynth" / "Reports"
    )
    p.add_argument("--rules", type=Path, default=Path.home() / "tools" / "sigma" / "rules")
    p.add_argument(
        "--mapping",
        type=Path,
        default=Path.home() / "tools" / "chainsaw" / "sigma-event-logs-all.yml",
    )
    p.add_argument("--prefer-logs", type=str, default="PowerShell-Operational.evtx,Security.evtx")

    p.add_argument("--chunk-model", default=DEFAULT_CHUNK_MODEL)
    p.add_argument("--final-model", default=DEFAULT_FINAL_MODEL)
    p.add_argument("--chunk-size", type=int, default=25)
    p.add_argument("--max-chunks", type=int, default=120)
    p.add_argument("--max-input-tokens", type=int, default=120_000)
    p.add_argument("--micro-truncate", type=int, default=200)
    p.add_argument("--micro-include-script", action="store_true")
    p.add_argument("--rpm", type=int, default=0)
    p.add_argument("--micro-workers", type=int, default=4 if os.cpu_count() else 2)
    p.add_argument("--llm-timeout", type=int, default=60)
    p.add_argument("--llm-max-retries", type=int, default=6)
    p.add_argument("--llm-temperature", type=float, default=0)

    a = p.parse_args()
    return AppConfig(
        two_pass=a.two_pass,
        fast=a.fast,
        make_html=a.make_html,
        branding=a.branding,
        toc=a.toc,
        evtx_root=a.evtx_root,
        out_root=a.out_root,
        rules_dir=a.rules,
        mapping_path=a.mapping,
        prefer_logs=[s.strip() for s in a.prefer_logs.split(",") if s.strip()],
        chunk_model=a.chunk_model,
        final_model=a.final_model,
        chunk_size=max(1, a.chunk_size),
        max_chunks=max(1, a.max_chunks),
        max_input_tokens=max(1000, a.max_input_tokens),
        micro_truncate=max(0, a.micro_truncate),
        micro_include_script=a.micro_include_script,
        rpm=max(0, a.rpm),
        micro_workers=max(1, a.micro_workers),
        llm_timeout=max(15, a.llm_timeout),
        llm_retries=max(1, a.llm_max_retries),
        temperature=a.llm_temperature,
    )


def main():
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        die("OPENAI_API_KEY not set")
    client = OpenAI(api_key=api_key)

    cfg = parse_args()

    # Boot-up splash (minimal)
    console.rule("🧠 ForenSynth AI — DFIR Intelligence Engine v2.0")
    if cfg.branding == "off":
        console.print(Panel.fit("Clean Report Mode — no branding footer added."))

    # Step 1 — Source
    src_dir, single = resolve_evtx_source(cfg.evtx_root, cfg.prefer_logs)

    # Output dir (timestamped, UTC)
    stamp = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H%M%SZ")
    run_dir = cfg.out_root / stamp
    run_dir.mkdir(parents=True, exist_ok=True)

    # Step 2 — Chainsaw
    start_ts = datetime.now(timezone.utc)
    info("Running Chainsaw hunt…")
    det_path = run_dir / "detections.json"
    det_count, _ = run_chainsaw(
        src_dir, det_path, cfg.rules_dir, cfg.mapping_path, single_file=single
    )

    # Load detections
    dets: List[Dict[str, Any]] = []
    if det_path.exists():
        try:
            dets = load_detections(det_path, max_items=0)
        except Exception:
            dets = []
    if not dets:
        console.rule("⚠ No Sigma detections found")
        console.print("No Sigma detections found — skipping summarization to save tokens.")
        # still write placeholder HTML/MD so repo is consistent
        html = assemble_html(
            "<p>No detections.</p>",
            "",
            "<p>None.</p>",
            {"labels": [], "values": [], "w": 24, "h": 1, "max": 0},
            0,
            cfg.chunk_model,
            cfg.final_model,
            0,
            0.0,
            start_ts.isoformat().replace("+00:00", "Z"),
            now_utc_iso(),
            cfg.branding,
            cfg.toc == "on",
        )
        html_path = write_html(run_dir, html)
        md_path = write_md(run_dir, "# ForenSynth Report\n\nNo detections.")
        ok(f"Empty report written: {html_path}")
        ok(f"Summary MD: {md_path}")
        return

    # Step 3 — LLM summarization
    # Live micro progress inside two_pass()
    md, usage = two_pass(client, dets, cfg)

    # Assemble HTML
    final_html = as_html(
        md.split("\n---\n\n## Appendix")[0].replace("# Final Executive Report", "")
    )
    micros_md = md.split("\n---\n\n## Appendix: Micro-Summaries\n\n")[-1]
    micros_html = as_html(micros_md)
    iocs = aggregate_iocs(dets)
    ioc_html = ioc_html_table(iocs)
    heat_json = build_heatmap(dets)

    end_ts = datetime.now(timezone.utc)
    runtime = int((end_ts - start_ts).total_seconds())
    cost, lines = estimate_cost(usage)

    html = assemble_html(
        final_html,
        micros_html,
        ioc_html,
        heat_json,
        len(dets),
        cfg.chunk_model,
        cfg.final_model,
        runtime,
        cost,
        start_ts.isoformat(timespec="seconds").replace("+00:00", "Z"),
        end_ts.isoformat(timespec="seconds").replace("+00:00", "Z"),
        cfg.branding,
        cfg.toc == "on",
    )

    html_path = write_html(run_dir, html)
    md_path = write_md(run_dir, md)

    console.print(Panel.fit(f"Report written: {html_path}"))
    console.print(Panel.fit(f"Summary MD:     {md_path}"))

    # Runtime footer
    console.rule("Runtime Summary")
    console.print(f"Started: {start_ts.isoformat().replace('+00:00', 'Z')}")
    console.print(f"Finished: {end_ts.isoformat().replace('+00:00', 'Z')}")
    console.print(f"Total: {runtime}s")
    console.rule("Cost Breakdown")
    for ln in lines:
        console.print(ln)
    console.print(f"Total cost: ${cost:.6f}")

    # Run log (autosort + pretty print last 5)
    log_path = write_csv_runlog(
        cfg.out_root,
        {
            "timestamp": end_ts.isoformat(timespec="seconds").replace("+00:00", "Z"),
            "detections": len(dets),
            "runtime_sec": runtime,
            "cost_usd": cost,
            "integrity": "off",
            "chunk_model": cfg.chunk_model.replace("gpt-", "5-")
            if cfg.chunk_model == "gpt-5-mini"
            else cfg.chunk_model,
            "final_model": cfg.final_model,
        },
    )
    ok(f"Run logged: {log_path}")


if __name__ == "__main__":
    main()
